{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import statistics\n",
    "import time\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Functions or Methods from Other Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from util.ipynb\n",
      "importing Jupyter notebook from model.ipynb\n",
      "importing Jupyter notebook from optimize_test.ipynb\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from optimize_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import None-Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NG_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(NG_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh',\n",
    "                                                input_shape=(dim_compress_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.model.add(self.att_layer1)\n",
    "\n",
    "        if dropout:\n",
    "            self.model.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer2)\n",
    "\n",
    "    def att_model(self):\n",
    "        attention_model = [self.compression_model, self.model]\n",
    "        return attention_model\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.att_model()[0](i)\n",
    "            h.append(c_imf)\n",
    "        \n",
    "        for i in x:\n",
    "            a = self.att_model()[1](i)\n",
    "            A.append(a)\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_Att_Net(tf.keras.Model):\n",
    "    def __init__(self, dim_features=1024, dim_compress_features=512, n_hidden_units=256, n_classes=2,\n",
    "                 dropout=False, dropout_rate=.25):\n",
    "        super(G_Att_Net, self).__init__()\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.compression_model = tf.keras.models.Sequential()\n",
    "        self.model1 = tf.keras.models.Sequential()\n",
    "        self.model2 = tf.keras.models.Sequential()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "\n",
    "        self.fc_compress_layer = tf.keras.layers.Dense(units=dim_compress_features, activation='relu',\n",
    "                                                       input_shape=(dim_features,), kernel_initializer='glorot_normal',\n",
    "                                                       bias_initializer='zeros', name='Fully_Connected_Layer')\n",
    "\n",
    "        self.compression_model.add(self.fc_compress_layer)\n",
    "        self.model1.add(self.fc_compress_layer)\n",
    "        self.model2.add(self.fc_compress_layer)\n",
    "\n",
    "        self.att_layer1 = tf.keras.layers.Dense(units=n_hidden_units, activation='tanh', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer1')\n",
    "\n",
    "        self.att_layer2 = tf.keras.layers.Dense(units=n_hidden_units, activation='sigmoid', input_shape=(dim_features,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer2')\n",
    "\n",
    "        self.att_layer3 = tf.keras.layers.Dense(units=n_classes, activation='linear', input_shape=(n_hidden_units,),\n",
    "                                                kernel_initializer='glorot_normal', bias_initializer='zeros',\n",
    "                                                name='Attention_Layer3')\n",
    "\n",
    "        self.model1.add(self.att_layer1)\n",
    "        self.model2.add(self.att_layer2)\n",
    "\n",
    "        if dropout:\n",
    "            self.model1.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "            self.model2.add(tf.keras.layers.Dropout(dropout_rate, name='Dropout_Layer'))\n",
    "\n",
    "        self.model.add(self.att_layer3)\n",
    "\n",
    "    def att_model(self):\n",
    "        attention_model = [self.compression_model, self.model1, self.model2, self.model]\n",
    "        return attention_model\n",
    "\n",
    "    def call(self, x):\n",
    "        h = list()\n",
    "        A = list()\n",
    "        \n",
    "        for i in x:\n",
    "            c_imf = self.att_model()[0](i)\n",
    "            h.append(c_imf)\n",
    "            \n",
    "        for i in x:\n",
    "            layer1_output = self.att_model()[1](i)  \n",
    "            layer2_output = self.att_model()[2](i)  \n",
    "            a = tf.math.multiply(layer1_output, layer2_output)  \n",
    "            a = self.att_model()[3](a)  \n",
    "            A.append(a)\n",
    "\n",
    "        return h, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Instance Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ins(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2, n_ins=8, mut_ex=False):\n",
    "        super(Ins, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "        self.n_ins = n_ins\n",
    "        self.mut_ex = mut_ex\n",
    "\n",
    "        self.ins_model = list()\n",
    "        self.m_ins_model = tf.keras.models.Sequential()\n",
    "        self.m_ins_layer = tf.keras.layers.Dense(\n",
    "            units=self.n_class, activation='linear', input_shape=(self.dim_compress_features,),\n",
    "            name='Instance_Classifier_Layer'\n",
    "        )\n",
    "        self.m_ins_model.add(self.m_ins_layer)\n",
    "\n",
    "        for i in range(self.n_class):\n",
    "            self.ins_model.append(self.m_ins_model)\n",
    "\n",
    "    def ins_classifier(self):\n",
    "        return self.ins_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pos_labels(n_pos_sample):\n",
    "        return tf.fill(dims=[n_pos_sample, ], value=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_neg_labels(n_neg_sample):\n",
    "        return tf.fill(dims=[n_neg_sample, ], value=0)\n",
    "    \n",
    "    def in_call(self, ins_classifier, h, A_I):\n",
    "        pos_label = self.generate_pos_labels(self.n_ins)\n",
    "        neg_label = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_in = tf.concat(values=[pos_label, neg_label], axis=0)\n",
    "        A_I = tf.reshape(tf.convert_to_tensor(A_I), (1, len(A_I))) \n",
    "        \n",
    "        top_pos_ids = tf.math.top_k(A_I, self.n_ins)[1][-1]  \n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    " \n",
    "        top_neg_ids = tf.math.top_k(-A_I, self.n_ins)[1][-1]\n",
    "        neg_index = list()\n",
    "        for i in top_neg_ids:\n",
    "             neg_index.append(i)\n",
    "\n",
    "        neg_index = tf.convert_to_tensor(neg_index)\n",
    "        top_neg = list()\n",
    "        for i in neg_index:\n",
    "            top_neg.append(h[i])\n",
    "\n",
    "        ins_in = tf.concat(values=[top_pos, top_neg], axis=0)\n",
    "        logits_unnorm_in = list()\n",
    "        logits_in = list()\n",
    "        \n",
    "        for i in range(self.n_class * self.n_ins):\n",
    "            ins_score_unnorm_in = ins_classifier(ins_in[i])\n",
    "            logit_in = tf.math.softmax(ins_score_unnorm_in)\n",
    "            logits_unnorm_in.append(ins_score_unnorm_in)\n",
    "            logits_in.append(logit_in)\n",
    "\n",
    "        return ins_label_in, logits_unnorm_in, logits_in\n",
    "    \n",
    "    def out_call(self, ins_classifier, h, A_O):\n",
    "        # get compressed 512-dimensional instance-level feature vectors for following use, denoted by h\n",
    "        A_O = tf.reshape(tf.convert_to_tensor(A_O), (1, len(A_O)))\n",
    "        top_pos_ids = tf.math.top_k(A_O, self.n_ins)[1][-1]\n",
    "        pos_index = list()\n",
    "        for i in top_pos_ids:\n",
    "            pos_index.append(i)\n",
    "\n",
    "        pos_index = tf.convert_to_tensor(pos_index)\n",
    "        top_pos = list()\n",
    "        for i in pos_index:\n",
    "            top_pos.append(h[i])\n",
    "\n",
    "        # mutually-exclusive -> top k instances w/ highest attention scores ==> false pos = neg\n",
    "        pos_ins_labels_out = self.generate_neg_labels(self.n_ins)\n",
    "        ins_label_out = pos_ins_labels_out\n",
    "        \n",
    "        logits_unnorm_out = list()\n",
    "        logits_out = list()\n",
    "  \n",
    "        for i in range(self.n_ins):\n",
    "            ins_score_unnorm_out = ins_classifier(top_pos[i])\n",
    "            logit_out = tf.math.softmax(ins_score_unnorm_out)\n",
    "            logits_unnorm_out.append(ins_score_unnorm_out)\n",
    "            logits_out.append(logit_out)\n",
    "\n",
    "        return ins_label_out, logits_unnorm_out, logits_out\n",
    "    \n",
    "    def call(self, bag_label, h, A):\n",
    "        for i in range(self.n_class):\n",
    "            ins_classifier = self.ins_classifier()[i]\n",
    "            if i == bag_label:\n",
    "                A_I = list()\n",
    "                for j in range(len(A)):\n",
    "                    a_i = A[j][0][i]\n",
    "                    A_I.append(a_i)\n",
    "                ins_label_in, logits_unnorm_in, logits_in = self.in_call(ins_classifier, h, A_I)\n",
    "            else:\n",
    "                if self.mut_ex:\n",
    "                    A_O = list()\n",
    "                    for j in range(len(A)):\n",
    "                        a_o = A[j][0][i]\n",
    "                        A_O.append(a_o)\n",
    "                    ins_label_out, logits_unnorm_out, logits_out = self.out_call(ins_classifier, h, A_O)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if self.mut_ex:\n",
    "            ins_labels = tf.concat(values=[ins_label_in, ins_label_out], axis=0)\n",
    "            ins_logits_unnorm = logits_unnorm_in + logits_unnorm_out\n",
    "            ins_logits = logits_in + logits_out\n",
    "        else:\n",
    "            ins_labels = ins_label_in\n",
    "            ins_logits_unnorm = logits_unnorm_in\n",
    "            ins_logits = logits_in\n",
    "        \n",
    "        return ins_labels, ins_logits_unnorm, ins_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bag Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(S_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.s_bag_model = tf.keras.models.Sequential()\n",
    "        self.s_bag_layer = tf.keras.layers.Dense(\n",
    "            units=1, activation='linear', input_shape=(self.n_class, self.dim_compress_features),\n",
    "            name='Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.s_bag_model.add(self.s_bag_layer)\n",
    "\n",
    "    def bag_classifier(self):\n",
    "        return self.s_bag_model\n",
    "\n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)   # return h_[slide,m], shape be (2,512)\n",
    "        \n",
    "        return slide_agg_rep\n",
    "    \n",
    "    def call(self, bag_label, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        bag_classifier = self.bag_classifier()\n",
    "        slide_score_unnorm = bag_classifier(slide_agg_rep)\n",
    "        slide_score_unnorm = tf.reshape(slide_score_unnorm, (1, self.n_class))\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm ,1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(tf.reshape(slide_score_unnorm, (1, self.n_class)))   #shape be (1,2), predictions for each of the classes\n",
    "        predict_slide_label = np.argmax(Y_prob.numpy())\n",
    "        \n",
    "        Y_true = tf.one_hot([bag_label], 2)\n",
    "\n",
    "        return slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Bag(tf.keras.Model):\n",
    "    def __init__(self, dim_compress_features=512, n_class=2):\n",
    "        super(M_Bag, self).__init__()\n",
    "        self.dim_compress_features = dim_compress_features\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.m_bag_models = list()\n",
    "        self.m_bag_model = tf.keras.models.Sequential() \n",
    "        self.m_bag_layer = tf.keras.layers.Dense(\n",
    "            units = 1, activation = 'linear', input_shape=(self.dim_compress_features,), name = 'Bag_Classifier_Layer'\n",
    "        )\n",
    "        self.m_bag_model.add(self.m_bag_layer)\n",
    "        for i in range(self.n_class):\n",
    "            self.m_bag_models.append(self.m_bag_model)\n",
    "            \n",
    "    def bag_classifier(self):       \n",
    "        return self.m_bag_models\n",
    "\n",
    "    def h_slide(self, A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)  # return h_[slide,m], shape be (2,512)\n",
    "\n",
    "        return slide_agg_rep\n",
    "\n",
    "    def in_call(self, bag_classifier, h_slide_I):\n",
    "        ssu_in = bag_classifier(h_slide_I)[0][0]\n",
    "\n",
    "        return ssu_in\n",
    "    \n",
    "    def out_call(self, bag_classifier, h_slide_O):\n",
    "        ssu_out = bag_classifier(h_slide_O)[0][0]\n",
    "        \n",
    "        return ssu_out\n",
    "    \n",
    "    def call(self, bag_label, A, h):\n",
    "        slide_agg_rep = self.h_slide(A, h)\n",
    "        # unnormalized slide-level score (s_[slide,m]) with uninitialized entries, shape be (1,num_of_classes)\n",
    "        slide_score_unnorm = tf.Variable(np.empty((1, self.n_class)), dtype=tf.float32)\n",
    "        slide_score_unnorm = tf.reshape(slide_score_unnorm, (1, self.n_class)).numpy()\n",
    " \n",
    "        # return s_[slide,m] (slide-level prediction scores)\n",
    "        for i in range(self.n_class):\n",
    "            bag_classifier = self.bag_classifier()[i]\n",
    "            if i == bag_label:\n",
    "                h_slide_I = tf.reshape(slide_agg_rep[i], (1, self.dim_compress_features))\n",
    "                ssu_in = self.in_call(bag_classifier, h_slide_I)\n",
    "            else:\n",
    "                h_slide_O = tf.reshape(slide_agg_rep[i], (1, self.dim_compress_features))\n",
    "                ssu_out = self.out_call(bag_classifier, h_slide_O)\n",
    "                \n",
    "        for i in range(self.n_class):\n",
    "            if i == bag_label:\n",
    "                slide_score_unnorm[0, i] = ssu_in\n",
    "            else:\n",
    "                slide_score_unnorm[0, i] = ssu_out\n",
    "        slide_score_unnorm = tf.convert_to_tensor(slide_score_unnorm)\n",
    "\n",
    "        Y_hat = tf.math.top_k(slide_score_unnorm, 1)[1][-1]\n",
    "        Y_prob = tf.math.softmax(slide_score_unnorm)\n",
    "        predict_slide_label = np.argmax(Y_prob.numpy())\n",
    "        \n",
    "        Y_true = tf.one_hot([bag_label], 2)\n",
    "\n",
    "        return slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_gate=False, net_size='small', n_ins=8, n_class=2, mut_ex=False, \n",
    "                 dropout=False, drop_rate=.25, mil_ins=False, att_only=False):\n",
    "        super(S_CLAM, self).__init__()\n",
    "        self.att_gate = att_gate\n",
    "        self.net_size = net_size\n",
    "        self.n_ins = n_ins\n",
    "        self.n_class = n_class\n",
    "        self.mut_ex = mut_ex\n",
    "        self.dropout = dropout\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mil_ins = mil_ins\n",
    "        self.att_only = att_only\n",
    "        \n",
    "        self.net_shape_dict = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        self.net_shape = self.net_shape_dict[self.net_size]\n",
    "        \n",
    "        if self.att_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1], n_hidden_units=self.net_shape[2],\n",
    "                                    n_classes=self.n_class, dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        \n",
    "        self.ins_net = Ins(dim_compress_features=self.net_shape[1], n_class=self.n_class, n_ins=self.n_ins, mut_ex=self.mut_ex)\n",
    "        \n",
    "        self.bag_net = S_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        \n",
    "    def clam_model(self):\n",
    "        att_model = self.att_net.att_model()\n",
    "        ins_classifier = self.ins_net.ins_classifier()\n",
    "        bag_classifier = self.bag_net.bag_classifier()\n",
    "        \n",
    "        clam_model = [att_model, ins_classifier, bag_classifier]\n",
    "        \n",
    "        return clam_model\n",
    "\n",
    "    def call(self, img_features, slide_label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            slide_label -> ground-truth slide label, could be 0 or 1 for binary classification\n",
    "        \"\"\"\n",
    "\n",
    "        h, A = self.att_net.call(img_features)\n",
    "        att_score = A  # output from attention network\n",
    "        A = tf.math.softmax(A)   # softmax onattention scores \n",
    "\n",
    "        if self.att_only:\n",
    "            return att_score\n",
    "        \n",
    "        if self.mil_ins:\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = self.ins_net.call(slide_label, h, A)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = self.bag_net.call(slide_label, A, h)\n",
    "\n",
    "        return att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, Y_prob, Y_hat, Y_true, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_CLAM(tf.keras.Model):\n",
    "    def __init__(self, att_gate=False, net_size='small', n_ins=8, n_class=2, mut_ex=False,\n",
    "                 dropout=False, drop_rate=.25, mil_ins=False, att_only=False):\n",
    "        super(M_CLAM, self).__init__()\n",
    "        self.att_gate = att_gate\n",
    "        self.net_size = net_size\n",
    "        self.n_ins = n_ins\n",
    "        self.n_class = n_class\n",
    "        self.mut_ex = mut_ex\n",
    "        self.dropout = dropout\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mil_ins = mil_ins\n",
    "        self.att_only = att_only\n",
    "\n",
    "        self.net_shape_dict = {\n",
    "            'small': [1024, 512, 256],\n",
    "            'big': [1024, 512, 384]\n",
    "        }\n",
    "        self.net_shape = self.net_shape_dict[self.net_size]\n",
    "\n",
    "        if self.att_gate:\n",
    "            self.att_net = G_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1],\n",
    "                                     n_hidden_units=self.net_shape[2], n_classes=self.n_class, \n",
    "                                     dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "        else:\n",
    "            self.att_net = NG_Att_Net(dim_features=self.net_shape[0], dim_compress_features=self.net_shape[1],\n",
    "                                      n_hidden_units=self.net_shape[2], n_classes=self.n_class, \n",
    "                                      dropout=self.dropout, dropout_rate=self.drop_rate)\n",
    "\n",
    "        self.ins_net = Ins(dim_compress_features=self.net_shape[1], n_class=self.n_class, \n",
    "                           n_ins=self.n_ins, mut_ex=self.mut_ex)\n",
    "        \n",
    "        self.bag_net = M_Bag(dim_compress_features=self.net_shape[1], n_class=self.n_class)\n",
    "        \n",
    "    def clam_model(self):\n",
    "        att_model = self.att_net.att_model()\n",
    "        ins_classifier = self.ins_net.ins_classifier()\n",
    "        bag_classifier = self.bag_net.bag_classifier()\n",
    "        \n",
    "        clam_model = [att_model, ins_classifier, bag_classifier]\n",
    "        \n",
    "        return clam_model\n",
    "    \n",
    "    def call(self, img_features, slide_label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_features -> original 1024-dimensional instance-level feature vectors\n",
    "            slide_label -> ground-truth slide label, could be 0 or 1 for binary classification\n",
    "        \"\"\"\n",
    "\n",
    "        h, A = self.att_net.call(img_features)\n",
    "        att_score = A  # output from attention network\n",
    "        A = tf.math.softmax(A)  # softmax onattention scores\n",
    "\n",
    "        if self.att_only:\n",
    "            return att_score\n",
    "\n",
    "        if self.mil_ins:\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = self.ins_net.call(slide_label, h, A)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = self.bag_net.call(slide_label, A, h)\n",
    "\n",
    "        return att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, Y_prob, Y_hat, Y_true, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
