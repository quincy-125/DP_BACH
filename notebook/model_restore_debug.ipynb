{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_labels(n_pos_sample):\n",
    "    return tf.fill(dims=[n_pos_sample, ], value=1)\n",
    "\n",
    "def generate_neg_labels(n_neg_sample):\n",
    "    return tf.fill(dims=[n_neg_sample, ], value=0)\n",
    "\n",
    "def ins_in_call(ins_classifier, h, A_I, n_ins, n_class):\n",
    "    pos_label = generate_pos_labels(n_ins)\n",
    "    neg_label = generate_neg_labels(n_ins)\n",
    "    ins_label_in = tf.concat(values=[pos_label, neg_label], axis=0)\n",
    "    A_I = tf.reshape(tf.convert_to_tensor(A_I), (1, len(A_I))) \n",
    "\n",
    "    top_pos_ids = tf.math.top_k(A_I, n_ins)[1][-1]  \n",
    "    pos_index = list()\n",
    "    for i in top_pos_ids:\n",
    "        pos_index.append(i)\n",
    "\n",
    "    pos_index = tf.convert_to_tensor(pos_index)\n",
    "    top_pos = list()\n",
    "    for i in pos_index:\n",
    "        top_pos.append(h[i])\n",
    "\n",
    "    top_neg_ids = tf.math.top_k(-A_I, n_ins)[1][-1]\n",
    "    neg_index = list()\n",
    "    for i in top_neg_ids:\n",
    "         neg_index.append(i)\n",
    "\n",
    "    neg_index = tf.convert_to_tensor(neg_index)\n",
    "    top_neg = list()\n",
    "    for i in neg_index:\n",
    "        top_neg.append(h[i])\n",
    "\n",
    "    ins_in = tf.concat(values=[top_pos, top_neg], axis=0)\n",
    "    logits_unnorm_in = list()\n",
    "    logits_in = list()\n",
    "\n",
    "    for i in range(n_class * n_ins):\n",
    "        ins_score_unnorm_in = ins_classifier(ins_in[i])\n",
    "        logit_in = tf.math.softmax(ins_score_unnorm_in)\n",
    "        logits_unnorm_in.append(ins_score_unnorm_in)\n",
    "        logits_in.append(logit_in)\n",
    "\n",
    "    return ins_label_in, logits_unnorm_in, logits_in\n",
    "\n",
    "def ins_out_call(ins_classifier, h, A_O, n_ins):\n",
    "    # get compressed 512-dimensional instance-level feature vectors for following use, denoted by h\n",
    "    A_O = tf.reshape(tf.convert_to_tensor(A_O), (1, len(A_O)))\n",
    "    top_pos_ids = tf.math.top_k(A_O, n_ins)[1][-1]\n",
    "    pos_index = list()\n",
    "    for i in top_pos_ids:\n",
    "        pos_index.append(i)\n",
    "\n",
    "    pos_index = tf.convert_to_tensor(pos_index)\n",
    "    top_pos = list()\n",
    "    for i in pos_index:\n",
    "        top_pos.append(h[i])\n",
    "\n",
    "    # mutually-exclusive -> top k instances w/ highest attention scores ==> false pos = neg\n",
    "    pos_ins_labels_out = generate_neg_labels(n_ins)\n",
    "    ins_label_out = pos_ins_labels_out\n",
    "\n",
    "    logits_unnorm_out = list()\n",
    "    logits_out = list()\n",
    "\n",
    "    for i in range(n_ins):\n",
    "        ins_score_unnorm_out = ins_classifier(top_pos[i])\n",
    "        logit_out = tf.math.softmax(ins_score_unnorm_out)\n",
    "        logits_unnorm_out.append(ins_score_unnorm_out)\n",
    "        logits_out.append(logit_out)\n",
    "\n",
    "    return ins_label_out, logits_unnorm_out, logits_out\n",
    "\n",
    "def ins_call(ins_classifiers, bag_label, h, A, n_class, n_ins, mut_ex):\n",
    "    for i in range(n_class):\n",
    "        ins_classifier = ins_classifiers[i]\n",
    "        if i == bag_label:\n",
    "            A_I = list()\n",
    "            for j in range(len(A)):\n",
    "                a_i = A[j][0][i]\n",
    "                A_I.append(a_i)\n",
    "            ins_label_in, logits_unnorm_in, logits_in = ins_in_call(ins_classifier, h, A_I, n_ins, n_class)\n",
    "        else:\n",
    "            if mut_ex:\n",
    "                A_O = list()\n",
    "                for j in range(len(A)):\n",
    "                    a_o = A[j][0][i]\n",
    "                    A_O.append(a_o)\n",
    "                ins_label_out, logits_unnorm_out, logits_out = ins_out_call(ins_classifier, h, A_O, n_ins)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    if mut_ex:\n",
    "        ins_labels = tf.concat(values=[ins_label_in, ins_label_out], axis=0)\n",
    "        ins_logits_unnorm = logits_unnorm_in + logits_unnorm_out\n",
    "        ins_logits = logits_in + logits_out\n",
    "    else:\n",
    "        ins_labels = ins_label_in\n",
    "        ins_logits_unnorm = logits_unnorm_in\n",
    "        ins_logits = logits_in\n",
    "\n",
    "    return ins_labels, ins_logits_unnorm, ins_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_bag_h_slide(A, h):\n",
    "        # compute the slide-level representation aggregated per the attention score distribution for the mth class\n",
    "        SAR = list()\n",
    "        for i in range(len(A)):\n",
    "            sar = tf.linalg.matmul(tf.transpose(A[i]), h[i])  # shape be (2,512)\n",
    "            SAR.append(sar)\n",
    "        slide_agg_rep = tf.math.add_n(SAR)   # return h_[slide,m], shape be (2,512)\n",
    "        \n",
    "        return slide_agg_rep\n",
    "    \n",
    "def s_bag_call(bag_classifier, bag_label, A, h, n_class):\n",
    "    slide_agg_rep = s_bag_h_slide(A, h)\n",
    "\n",
    "    slide_score_unnorm = bag_classifier(slide_agg_rep)\n",
    "    slide_score_unnorm = tf.reshape(slide_score_unnorm, (1, n_class))\n",
    "    Y_hat = tf.math.top_k(slide_score_unnorm ,1)[1][-1]\n",
    "    Y_prob = tf.math.softmax(tf.reshape(slide_score_unnorm, (1, n_class)))   #shape be (1,2), predictions for each of the classes\n",
    "    predict_slide_label = np.argmax(Y_prob.numpy())\n",
    "\n",
    "    Y_true = tf.one_hot([bag_label], 2)\n",
    "\n",
    "    return slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_att_call(g_att_net, img_features):\n",
    "    h = list()\n",
    "    A = list()\n",
    "\n",
    "    for i in img_features:\n",
    "        c_imf = g_att_net[0](i)\n",
    "        h.append(c_imf)\n",
    "\n",
    "    for i in img_features:\n",
    "        layer1_output = g_att_net[1](i)\n",
    "        layer2_output = g_att_net[2](i)\n",
    "        a = tf.math.multiply(layer1_output, layer2_output)\n",
    "        a = g_att_net[3](a)\n",
    "        A.append(a)\n",
    "        \n",
    "    return h, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_clam_call(att_net, ins_net, bag_net, img_features, slide_label, n_class, n_ins, att_only, mil_ins, mut_ex):\n",
    "    h, A = g_att_call(att_net, img_features)\n",
    "    att_score = A  # output from attention network\n",
    "    A = tf.math.softmax(A)   # softmax onattention scores \n",
    "\n",
    "    if att_only:\n",
    "        return att_score\n",
    "\n",
    "    if mil_ins:\n",
    "        ins_labels, ins_logits_unnorm, ins_logits = ins_call(ins_net, slide_label, h, A, n_class, n_ins, False)\n",
    "\n",
    "    slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = bag_call(bag_net, slide_label, A, h, n_class)\n",
    "\n",
    "    return att_score, A, h, ins_labels, ins_logits_unnorm, ins_logits, slide_score_unnorm, Y_prob, Y_hat, Y_true, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
