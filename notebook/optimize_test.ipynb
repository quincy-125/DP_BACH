{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import statistics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Functions or Methods from Other Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CLAM Model on the Given Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_optimize(\n",
    "    img_features,\n",
    "    slide_label,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_optimizer,\n",
    "    b_optimizer,\n",
    "    c_optimizer,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    mutual_ex,\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "\n",
    "        (\n",
    "            att_score,\n",
    "            A,\n",
    "            h,\n",
    "            ins_labels,\n",
    "            ins_logits_unnorm,\n",
    "            ins_logits,\n",
    "            slide_score_unnorm,\n",
    "            Y_prob,\n",
    "            Y_hat,\n",
    "            Y_true,\n",
    "            predict_slide_label,\n",
    "        ) = c_model.call(img_features, slide_label)\n",
    "\n",
    "        ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "        ins_loss = list()\n",
    "        for j in range(len(ins_logits)):\n",
    "            i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "            ins_loss.append(i_loss)\n",
    "        if mutual_ex:\n",
    "            I_Loss = tf.math.add_n(ins_loss) / n_class\n",
    "        else:\n",
    "            I_Loss = tf.math.add_n(ins_loss)\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = b_model.call(\n",
    "            slide_label, A, h\n",
    "        )\n",
    "\n",
    "        B_Loss = b_loss_func(Y_true, Y_prob)\n",
    "\n",
    "        T_Loss = c1 * B_Loss + c2 * I_Loss\n",
    "\n",
    "    i_grad = i_tape.gradient(I_Loss, i_model.trainable_weights)\n",
    "    i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "    b_grad = b_tape.gradient(B_Loss, b_model.trainable_weights)\n",
    "    b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "    c_grad = c_tape.gradient(T_Loss, c_model.trainable_weights)\n",
    "    c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "\n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_optimize(\n",
    "    batch_size,\n",
    "    n_ins,\n",
    "    n_samples,\n",
    "    img_features,\n",
    "    slide_label,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_optimizer,\n",
    "    b_optimizer,\n",
    "    c_optimizer,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    mutual_ex,\n",
    "):\n",
    "\n",
    "    step_size = 0\n",
    "\n",
    "    Ins_Loss = list()\n",
    "    Bag_Loss = list()\n",
    "    Total_Loss = list()\n",
    "\n",
    "    label_predict = list()\n",
    "\n",
    "    for n_step in range(0, (n_samples // batch_size + 1)):\n",
    "        if step_size < (n_samples - batch_size):\n",
    "            with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "                (\n",
    "                    att_score,\n",
    "                    A,\n",
    "                    h,\n",
    "                    ins_labels,\n",
    "                    ins_logits_unnorm,\n",
    "                    ins_logits,\n",
    "                    slide_score_unnorm,\n",
    "                    Y_prob,\n",
    "                    Y_hat,\n",
    "                    Y_true,\n",
    "                    predict_label,\n",
    "                ) = c_model.call(\n",
    "                    img_features=img_features[step_size : (step_size + batch_size)],\n",
    "                    slide_label=slide_label,\n",
    "                )\n",
    "\n",
    "                ins_labels, ins_logits_unnorm, ins_logits = i_model.call(\n",
    "                    slide_label, h, A\n",
    "                )\n",
    "\n",
    "                ins_loss = list()\n",
    "                for j in range(len(ins_logits)):\n",
    "                    i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                    ins_loss.append(i_loss)\n",
    "                if mutual_ex:\n",
    "                    Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "                else:\n",
    "                    Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "                slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(\n",
    "                    slide_label, A, h\n",
    "                )\n",
    "\n",
    "                Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "\n",
    "                Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "            i_grad = i_tape.gradient(Loss_I, i_model.trainable_weights)\n",
    "            i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "            b_grad = b_tape.gradient(Loss_B, b_model.trainable_weights)\n",
    "            b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "            c_grad = c_tape.gradient(Loss_T, c_model.trainable_weights)\n",
    "            c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "\n",
    "        else:\n",
    "            with tf.GradientTape() as i_tape, tf.GradientTape() as b_tape, tf.GradientTape() as c_tape:\n",
    "                (\n",
    "                    att_score,\n",
    "                    A,\n",
    "                    h,\n",
    "                    ins_labels,\n",
    "                    ins_logits_unnorm,\n",
    "                    ins_logits,\n",
    "                    slide_score_unnorm,\n",
    "                    Y_prob,\n",
    "                    Y_hat,\n",
    "                    Y_true,\n",
    "                    predict_label,\n",
    "                ) = c_model.call(\n",
    "                    img_features=img_features[(step_size - n_ins) :],\n",
    "                    slide_label=slide_label,\n",
    "                )\n",
    "\n",
    "                ins_labels, ins_logits_unnorm, ins_logits = i_model.call(\n",
    "                    slide_label, h, A\n",
    "                )\n",
    "\n",
    "                ins_loss = list()\n",
    "                for j in range(len(ins_logits)):\n",
    "                    i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                    ins_loss.append(i_loss)\n",
    "                if mutual_ex:\n",
    "                    Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "                else:\n",
    "                    Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "                slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(\n",
    "                    slide_label, A, h\n",
    "                )\n",
    "\n",
    "                Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "\n",
    "                Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "            i_grad = i_tape.gradient(Loss_I, i_model.trainable_weights)\n",
    "            i_optimizer.apply_gradients(zip(i_grad, i_model.trainable_weights))\n",
    "\n",
    "            b_grad = b_tape.gradient(Loss_B, b_model.trainable_weights)\n",
    "            b_optimizer.apply_gradients(zip(b_grad, b_model.trainable_weights))\n",
    "\n",
    "            c_grad = c_tape.gradient(Loss_T, c_model.trainable_weights)\n",
    "            c_optimizer.apply_gradients(zip(c_grad, c_model.trainable_weights))\n",
    "\n",
    "        Ins_Loss.append(float(Loss_I))\n",
    "        Bag_Loss.append(float(Loss_B))\n",
    "        Total_Loss.append(float(Loss_T))\n",
    "\n",
    "        label_predict.append(predict_label)\n",
    "\n",
    "        step_size += batch_size\n",
    "\n",
    "    I_Loss = statistics.mean(Ins_Loss)\n",
    "    B_Loss = statistics.mean(Bag_Loss)\n",
    "    T_Loss = statistics.mean(Total_Loss)\n",
    "\n",
    "    predict_slide_label = most_frequent(label_predict)\n",
    "\n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    train_path,\n",
    "    i_optimizer_func,\n",
    "    b_optimizer_func,\n",
    "    c_optimizer_func,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    mutual_ex,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    i_learn_rate,\n",
    "    b_learn_rate,\n",
    "    c_learn_rate,\n",
    "    i_l2_decay,\n",
    "    b_l2_decay,\n",
    "    c_l2_decay,\n",
    "    n_ins,\n",
    "    batch_size,\n",
    "    batch_op,\n",
    "):\n",
    "\n",
    "    loss_total = list()\n",
    "    loss_ins = list()\n",
    "    loss_bag = list()\n",
    "\n",
    "    i_optimizer = i_optimizer_func(learning_rate=i_learn_rate, weight_decay=i_l2_decay)\n",
    "    b_optimizer = b_optimizer_func(learning_rate=b_learn_rate, weight_decay=b_l2_decay)\n",
    "    c_optimizer = c_optimizer_func(learning_rate=c_learn_rate, weight_decay=c_l2_decay)\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "\n",
    "    train_sample_list = os.listdir(train_path)\n",
    "    train_sample_list = random.sample(train_sample_list, len(train_sample_list))\n",
    "    for i in train_sample_list:\n",
    "        print(\"=\", end=\"\")\n",
    "        single_train_data = train_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_train_data)\n",
    "        # shuffle the order of img features list in order to reduce the side effects of randomly drop potential\n",
    "        # number of patches' feature vectors during training when enable batch training option\n",
    "        img_features = random.sample(img_features, len(img_features))\n",
    "\n",
    "        if batch_op:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = b_optimize(\n",
    "                batch_size=batch_size,\n",
    "                n_ins=n_ins,\n",
    "                n_samples=len(img_features),\n",
    "                img_features=img_features,\n",
    "                slide_label=slide_label,\n",
    "                i_model=i_model,\n",
    "                b_model=b_model,\n",
    "                c_model=c_model,\n",
    "                i_optimizer=i_optimizer,\n",
    "                b_optimizer=b_optimizer,\n",
    "                c_optimizer=c_optimizer,\n",
    "                i_loss_func=i_loss_func,\n",
    "                b_loss_func=b_loss_func,\n",
    "                n_class=n_class,\n",
    "                c1=c1,\n",
    "                c2=c2,\n",
    "                mutual_ex=mutual_ex,\n",
    "            )\n",
    "        else:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = nb_optimize(\n",
    "                img_features=img_features,\n",
    "                slide_label=slide_label,\n",
    "                i_model=i_model,\n",
    "                b_model=b_model,\n",
    "                c_model=c_model,\n",
    "                i_optimizer=i_optimizer,\n",
    "                b_optimizer=b_optimizer,\n",
    "                c_optimizer=c_optimizer,\n",
    "                i_loss_func=i_loss_func,\n",
    "                b_loss_func=b_loss_func,\n",
    "                n_class=n_class,\n",
    "                c1=c1,\n",
    "                c2=c2,\n",
    "                mutual_ex=mutual_ex,\n",
    "            )\n",
    "\n",
    "        loss_total.append(float(T_Loss))\n",
    "        loss_ins.append(float(I_Loss))\n",
    "        loss_bag.append(float(B_Loss))\n",
    "\n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "        slide_true_label, slide_predict_label\n",
    "    ).ravel()\n",
    "    train_tn = int(tn)\n",
    "    train_fp = int(fp)\n",
    "    train_fn = int(fn)\n",
    "    train_tp = int(tp)\n",
    "\n",
    "    train_sensitivity = round(train_tp / (train_tp + train_fn), 2)\n",
    "    train_specificity = round(train_tn / (train_tn + train_fp), 2)\n",
    "    train_acc = round(\n",
    "        (train_tp + train_tn) / (train_tn + train_fp + train_fn + train_tp), 2\n",
    "    )\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(\n",
    "        slide_true_label, slide_predict_label, pos_label=1\n",
    "    )\n",
    "    train_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    train_loss = statistics.mean(loss_total)\n",
    "    train_ins_loss = statistics.mean(loss_ins)\n",
    "    train_bag_loss = statistics.mean(loss_bag)\n",
    "\n",
    "    return (\n",
    "        train_loss,\n",
    "        train_ins_loss,\n",
    "        train_bag_loss,\n",
    "        train_tn,\n",
    "        train_fp,\n",
    "        train_fn,\n",
    "        train_tp,\n",
    "        train_sensitivity,\n",
    "        train_specificity,\n",
    "        train_acc,\n",
    "        train_auc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_val(\n",
    "    img_features,\n",
    "    slide_label,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    mutual_ex,\n",
    "):\n",
    "\n",
    "    (\n",
    "        att_score,\n",
    "        A,\n",
    "        h,\n",
    "        ins_labels,\n",
    "        ins_logits_unnorm,\n",
    "        ins_logits,\n",
    "        slide_score_unnorm,\n",
    "        Y_prob,\n",
    "        Y_hat,\n",
    "        Y_true,\n",
    "        predict_slide_label,\n",
    "    ) = c_model.call(img_features, slide_label)\n",
    "\n",
    "    ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "\n",
    "    ins_loss = list()\n",
    "    for j in range(len(ins_logits)):\n",
    "        i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "        ins_loss.append(i_loss)\n",
    "    if mutual_ex:\n",
    "        I_Loss = tf.math.add_n(ins_loss) / n_class\n",
    "    else:\n",
    "        I_Loss = tf.math.add_n(ins_loss)\n",
    "\n",
    "    slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = b_model.call(\n",
    "        slide_label, A, h\n",
    "    )\n",
    "\n",
    "    B_Loss = b_loss_func(Y_true, Y_prob)\n",
    "\n",
    "    T_Loss = c1 * B_Loss + c2 * I_Loss\n",
    "\n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_val(\n",
    "    batch_size,\n",
    "    n_ins,\n",
    "    n_samples,\n",
    "    img_features,\n",
    "    slide_label,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    mutual_ex,\n",
    "):\n",
    "\n",
    "    step_size = 0\n",
    "\n",
    "    Ins_Loss = list()\n",
    "    Bag_Loss = list()\n",
    "    Total_Loss = list()\n",
    "\n",
    "    label_predict = list()\n",
    "\n",
    "    for n_step in range(0, (n_samples // batch_size + 1)):\n",
    "        if step_size < (n_samples - batch_size):\n",
    "            (\n",
    "                att_score,\n",
    "                A,\n",
    "                h,\n",
    "                ins_labels,\n",
    "                ins_logits_unnorm,\n",
    "                ins_logits,\n",
    "                slide_score_unnorm,\n",
    "                Y_prob,\n",
    "                Y_hat,\n",
    "                Y_true,\n",
    "                predict_label,\n",
    "            ) = c_model.call(\n",
    "                img_features=img_features[step_size : (step_size + batch_size)],\n",
    "                slide_label=slide_label,\n",
    "            )\n",
    "\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "\n",
    "            ins_loss = list()\n",
    "            for j in range(len(ins_logits)):\n",
    "                i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                ins_loss.append(i_loss)\n",
    "            if mutual_ex:\n",
    "                Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "            else:\n",
    "                Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "            slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(\n",
    "                slide_label, A, h\n",
    "            )\n",
    "\n",
    "            Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "            Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "        else:\n",
    "            (\n",
    "                att_score,\n",
    "                A,\n",
    "                h,\n",
    "                ins_labels,\n",
    "                ins_logits_unnorm,\n",
    "                ins_logits,\n",
    "                slide_score_unnorm,\n",
    "                Y_prob,\n",
    "                Y_hat,\n",
    "                Y_true,\n",
    "                predict_label,\n",
    "            ) = c_model.call(\n",
    "                img_features=img_features[(step_size - n_ins) :],\n",
    "                slide_label=slide_label,\n",
    "            )\n",
    "\n",
    "            ins_labels, ins_logits_unnorm, ins_logits = i_model.call(slide_label, h, A)\n",
    "\n",
    "            ins_loss = list()\n",
    "            for j in range(len(ins_logits)):\n",
    "                i_loss = i_loss_func(tf.one_hot(ins_labels[j], 2), ins_logits[j])\n",
    "                ins_loss.append(i_loss)\n",
    "            if mutual_ex:\n",
    "                Loss_I = tf.math.add_n(ins_loss) / n_class\n",
    "            else:\n",
    "                Loss_I = tf.math.add_n(ins_loss)\n",
    "\n",
    "            slide_score_unnorm, Y_hat, Y_prob, predict_label, Y_true = b_model.call(\n",
    "                slide_label, A, h\n",
    "            )\n",
    "\n",
    "            Loss_B = b_loss_func(Y_true, Y_prob)\n",
    "\n",
    "            Loss_T = c1 * Loss_B + c2 * Loss_I\n",
    "\n",
    "        Ins_Loss.append(float(Loss_I))\n",
    "        Bag_Loss.append(float(Loss_B))\n",
    "        Total_Loss.append(float(Loss_T))\n",
    "\n",
    "        label_predict.append(predict_label)\n",
    "\n",
    "        step_size += batch_size\n",
    "\n",
    "    I_Loss = statistics.mean(Ins_Loss)\n",
    "    B_Loss = statistics.mean(Bag_Loss)\n",
    "    T_Loss = statistics.mean(Total_Loss)\n",
    "\n",
    "    predict_slide_label = most_frequent(label_predict)\n",
    "\n",
    "    return I_Loss, B_Loss, T_Loss, predict_slide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    val_path,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    mutual_ex,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    n_ins,\n",
    "    batch_size,\n",
    "    batch_op,\n",
    "):\n",
    "    loss_t = list()\n",
    "    loss_i = list()\n",
    "    loss_b = list()\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "\n",
    "    val_sample_list = os.listdir(val_path)\n",
    "    val_sample_list = random.sample(val_sample_list, len(val_sample_list))\n",
    "    for i in val_sample_list:\n",
    "        print(\"=\", end=\"\")\n",
    "        single_val_data = val_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_val_data)\n",
    "\n",
    "        img_features = random.sample(\n",
    "            img_features, len(img_features)\n",
    "        )  # follow the training loop, see details there\n",
    "\n",
    "        if batch_op:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = b_val(\n",
    "                batch_size=batch_size,\n",
    "                n_ins=n_ins,\n",
    "                n_samples=len(img_features),\n",
    "                img_features=img_features,\n",
    "                slide_label=slide_label,\n",
    "                i_model=i_model,\n",
    "                b_model=b_model,\n",
    "                c_model=c_model,\n",
    "                i_loss_func=i_loss_func,\n",
    "                b_loss_func=b_loss_func,\n",
    "                n_class=n_class,\n",
    "                c1=c1,\n",
    "                c2=c2,\n",
    "                mutual_ex=mutual_ex,\n",
    "            )\n",
    "        else:\n",
    "            I_Loss, B_Loss, T_Loss, predict_slide_label = nb_val(\n",
    "                img_features=img_features,\n",
    "                slide_label=slide_label,\n",
    "                i_model=i_model,\n",
    "                b_model=b_model,\n",
    "                c_model=c_model,\n",
    "                i_loss_func=i_loss_func,\n",
    "                b_loss_func=b_loss_func,\n",
    "                n_class=n_class,\n",
    "                c1=c1,\n",
    "                c2=c2,\n",
    "                mutual_ex=mutual_ex,\n",
    "            )\n",
    "\n",
    "        loss_t.append(float(T_Loss))\n",
    "        loss_i.append(float(I_Loss))\n",
    "        loss_b.append(float(B_Loss))\n",
    "\n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "        slide_true_label, slide_predict_label\n",
    "    ).ravel()\n",
    "    val_tn = int(tn)\n",
    "    val_fp = int(fp)\n",
    "    val_fn = int(fn)\n",
    "    val_tp = int(tp)\n",
    "\n",
    "    val_sensitivity = round(val_tp / (val_tp + val_fn), 2)\n",
    "    val_specificity = round(val_tn / (val_tn + val_fp), 2)\n",
    "    val_acc = round((val_tp + val_tn) / (val_tn + val_fp + val_fn + val_tp), 2)\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(\n",
    "        slide_true_label, slide_predict_label, pos_label=1\n",
    "    )\n",
    "    val_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    val_loss = statistics.mean(loss_t)\n",
    "    val_ins_loss = statistics.mean(loss_i)\n",
    "    val_bag_loss = statistics.mean(loss_b)\n",
    "\n",
    "    return (\n",
    "        val_loss,\n",
    "        val_ins_loss,\n",
    "        val_bag_loss,\n",
    "        val_tn,\n",
    "        val_fp,\n",
    "        val_fn,\n",
    "        val_tp,\n",
    "        val_sensitivity,\n",
    "        val_specificity,\n",
    "        val_acc,\n",
    "        val_auc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Optimized CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    n_class,\n",
    "    n_ins,\n",
    "    att_gate,\n",
    "    att_only,\n",
    "    mil_ins,\n",
    "    mut_ex,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    test_path,\n",
    "    result_path,\n",
    "    result_file_name,\n",
    "):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    slide_true_label = list()\n",
    "    slide_predict_label = list()\n",
    "    sample_names = list()\n",
    "\n",
    "    for i in os.listdir(test_path):\n",
    "        print(\">\", end=\"\")\n",
    "        single_test_data = test_path + i\n",
    "        img_features, slide_label = get_data_from_tf(single_test_data)\n",
    "\n",
    "        (\n",
    "            att_score,\n",
    "            A,\n",
    "            h,\n",
    "            ins_labels,\n",
    "            ins_logits_unnorm,\n",
    "            ins_logits,\n",
    "            slide_score_unnorm,\n",
    "            Y_prob,\n",
    "            Y_hat,\n",
    "            Y_true,\n",
    "            predict_slide_label,\n",
    "        ) = s_clam_call(\n",
    "            att_net=c_model[0],\n",
    "            ins_net=c_model[1],\n",
    "            bag_net=c_model[2],\n",
    "            img_features=img_features,\n",
    "            slide_label=slide_label,\n",
    "            n_class=n_class,\n",
    "            n_ins=n_ins,\n",
    "            att_gate=att_gate,\n",
    "            att_only=att_only,\n",
    "            mil_ins=mil_ins,\n",
    "            mut_ex=mut_ex,\n",
    "        )\n",
    "\n",
    "        ins_labels, ins_logits_unnorm, ins_logits = ins_call(\n",
    "            m_ins_classifier=i_model,\n",
    "            bag_label=slide_label,\n",
    "            h=h,\n",
    "            A=A,\n",
    "            n_class=n_class,\n",
    "            n_ins=n_ins,\n",
    "            mut_ex=mut_ex,\n",
    "        )\n",
    "\n",
    "        slide_score_unnorm, Y_hat, Y_prob, predict_slide_label, Y_true = s_bag_call(\n",
    "            bag_classifier=b_model, bag_label=slide_label, A=A, h=h, n_class=n_class\n",
    "        )\n",
    "\n",
    "        slide_true_label.append(slide_label)\n",
    "        slide_predict_label.append(predict_slide_label)\n",
    "        sample_names.append(i)\n",
    "\n",
    "        test_results = pd.DataFrame(\n",
    "            list(zip(sample_names, slide_true_label, slide_predict_label)),\n",
    "            columns=[\"Sample Names\", \"Slide True Label\", \"Slide Predict Label\"],\n",
    "        )\n",
    "        test_results.to_csv(\n",
    "            os.path.join(result_path, result_file_name), sep=\"\\t\", index=False\n",
    "        )\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "        slide_true_label, slide_predict_label\n",
    "    ).ravel()\n",
    "    test_tn = int(tn)\n",
    "    test_fp = int(fp)\n",
    "    test_fn = int(fn)\n",
    "    test_tp = int(tp)\n",
    "\n",
    "    test_sensitivity = round(test_tp / (test_tp + test_fn), 2)\n",
    "    test_specificity = round(test_tn / (test_tn + test_fp), 2)\n",
    "    test_acc = round((test_tp + test_tn) / (test_tn + test_fp + test_fn + test_tp), 2)\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(\n",
    "        slide_true_label, slide_predict_label, pos_label=1\n",
    "    )\n",
    "    test_auc = round(sklearn.metrics.auc(fpr, tpr), 2)\n",
    "\n",
    "    test_run_time = time.time() - start_time\n",
    "\n",
    "    template = \"\\n Test Accuracy: {}, Test Sensitivity: {}, Test Specificity: {}, Test Running Time: {}\"\n",
    "    print(\n",
    "        template.format(\n",
    "            f\"{float(test_acc):.4%}\",\n",
    "            f\"{float(test_sensitivity):.4%}\",\n",
    "            f\"{float(test_specificity):.4%}\",\n",
    "            \"--- %s mins ---\" % int(test_run_time / 60),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validating CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(\n",
    "    train_log,\n",
    "    val_log,\n",
    "    train_path,\n",
    "    val_path,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_optimizer_func,\n",
    "    b_optimizer_func,\n",
    "    c_optimizer_func,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    mutual_ex,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    i_learn_rate,\n",
    "    b_learn_rate,\n",
    "    c_learn_rate,\n",
    "    i_l2_decay,\n",
    "    b_l2_decay,\n",
    "    c_l2_decay,\n",
    "    n_ins,\n",
    "    batch_size,\n",
    "    batch_op,\n",
    "    epochs,\n",
    "):\n",
    "\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Step\n",
    "        start_time = time.time()\n",
    "\n",
    "        (\n",
    "            train_loss,\n",
    "            train_ins_loss,\n",
    "            train_bag_loss,\n",
    "            train_tn,\n",
    "            train_fp,\n",
    "            train_fn,\n",
    "            train_tp,\n",
    "            train_sensitivity,\n",
    "            train_specificity,\n",
    "            train_acc,\n",
    "            train_auc,\n",
    "        ) = train_step(\n",
    "            i_model=i_model,\n",
    "            b_model=b_model,\n",
    "            c_model=c_model,\n",
    "            train_path=train_path,\n",
    "            i_optimizer_func=i_optimizer_func,\n",
    "            b_optimizer_func=b_optimizer_func,\n",
    "            c_optimizer_func=c_optimizer_func,\n",
    "            i_loss_func=i_loss_func,\n",
    "            b_loss_func=b_loss_func,\n",
    "            mutual_ex=mutual_ex,\n",
    "            n_class=n_class,\n",
    "            c1=c1,\n",
    "            c2=c2,\n",
    "            i_learn_rate=i_learn_rate,\n",
    "            b_learn_rate=b_learn_rate,\n",
    "            c_learn_rate=c_learn_rate,\n",
    "            i_l2_decay=i_l2_decay,\n",
    "            b_l2_decay=b_l2_decay,\n",
    "            c_l2_decay=c_l2_decay,\n",
    "            n_ins=n_ins,\n",
    "            batch_size=batch_size,\n",
    "            batch_op=batch_op,\n",
    "        )\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Total Loss\", float(train_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Instance Loss\", float(train_ins_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Bag Loss\", float(train_bag_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Accuracy\", float(train_acc), step=epoch)\n",
    "            tf.summary.scalar(\"AUC\", float(train_auc), step=epoch)\n",
    "            tf.summary.scalar(\"Sensitivity\", float(train_sensitivity), step=epoch)\n",
    "            tf.summary.scalar(\"Specificity\", float(train_specificity), step=epoch)\n",
    "            tf.summary.histogram(\"True Positive\", int(train_tp), step=epoch)\n",
    "            tf.summary.histogram(\"False Positive\", int(train_fp), step=epoch)\n",
    "            tf.summary.histogram(\"True Negative\", int(train_tn), step=epoch)\n",
    "            tf.summary.histogram(\"False Negative\", int(train_fn), step=epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        (\n",
    "            val_loss,\n",
    "            val_ins_loss,\n",
    "            val_bag_loss,\n",
    "            val_tn,\n",
    "            val_fp,\n",
    "            val_fn,\n",
    "            val_tp,\n",
    "            val_sensitivity,\n",
    "            val_specificity,\n",
    "            val_acc,\n",
    "            val_auc,\n",
    "        ) = val_step(\n",
    "            i_model=i_model,\n",
    "            b_model=b_model,\n",
    "            c_model=c_model,\n",
    "            val_path=val_path,\n",
    "            i_loss_func=i_loss_func,\n",
    "            b_loss_func=b_loss_func,\n",
    "            mutual_ex=mutual_ex,\n",
    "            n_class=n_class,\n",
    "            c1=c1,\n",
    "            c2=c2,\n",
    "            n_ins=n_ins,\n",
    "            batch_size=batch_size,\n",
    "            batch_op=batch_op,\n",
    "        )\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Total Loss\", float(val_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Instance Loss\", float(val_ins_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Bag Loss\", float(val_bag_loss), step=epoch)\n",
    "            tf.summary.scalar(\"Accuracy\", float(val_acc), step=epoch)\n",
    "            tf.summary.scalar(\"AUC\", float(val_auc), step=epoch)\n",
    "            tf.summary.scalar(\"Sensitivity\", float(val_sensitivity), step=epoch)\n",
    "            tf.summary.scalar(\"Specificity\", float(val_specificity), step=epoch)\n",
    "            tf.summary.histogram(\"True Positive\", int(val_tp), step=epoch)\n",
    "            tf.summary.histogram(\"False Positive\", int(val_fp), step=epoch)\n",
    "            tf.summary.histogram(\"True Negative\", int(val_tn), step=epoch)\n",
    "            tf.summary.histogram(\"False Negative\", int(val_fn), step=epoch)\n",
    "\n",
    "        epoch_run_time = time.time() - start_time\n",
    "        template = (\n",
    "            \"\\n Epoch {},  Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}, Epoch Running \"\n",
    "            \"Time: {} \"\n",
    "        )\n",
    "        print(\n",
    "            template.format(\n",
    "                epoch + 1,\n",
    "                f\"{float(train_loss):.8}\",\n",
    "                f\"{float(train_acc):.4%}\",\n",
    "                f\"{float(val_loss):.8}\",\n",
    "                f\"{float(val_acc):.4%}\",\n",
    "                \"--- %s mins ---\" % int(epoch_run_time / 60),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function to Optimizing and Testing CLAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Optimized CLAM Model by Saving the Trained CLAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clam_optimize(\n",
    "    train_log,\n",
    "    val_log,\n",
    "    train_path,\n",
    "    val_path,\n",
    "    i_model,\n",
    "    b_model,\n",
    "    c_model,\n",
    "    i_optimizer_func,\n",
    "    b_optimizer_func,\n",
    "    c_optimizer_func,\n",
    "    i_loss_func,\n",
    "    b_loss_func,\n",
    "    mutual_ex,\n",
    "    n_class,\n",
    "    c1,\n",
    "    c2,\n",
    "    i_learn_rate,\n",
    "    b_learn_rate,\n",
    "    c_learn_rate,\n",
    "    i_l2_decay,\n",
    "    b_l2_decay,\n",
    "    c_l2_decay,\n",
    "    n_ins,\n",
    "    batch_size,\n",
    "    batch_op,\n",
    "    i_model_dir,\n",
    "    b_model_dir,\n",
    "    c_model_dir,\n",
    "    m_bag_op,\n",
    "    m_clam_op,\n",
    "    g_att_op,\n",
    "    epochs,\n",
    "):\n",
    "\n",
    "    train_val(\n",
    "        train_log=train_log,\n",
    "        val_log=val_log,\n",
    "        train_path=train_path,\n",
    "        val_path=val_path,\n",
    "        i_model=i_model,\n",
    "        b_model=b_model,\n",
    "        c_model=c_model,\n",
    "        i_optimizer_func=i_optimizer_func,\n",
    "        b_optimizer_func=b_optimizer_func,\n",
    "        c_optimizer_func=c_optimizer_func,\n",
    "        i_loss_func=i_loss_func,\n",
    "        b_loss_func=b_loss_func,\n",
    "        mutual_ex=mutual_ex,\n",
    "        n_class=n_class,\n",
    "        c1=c1,\n",
    "        c2=c2,\n",
    "        i_learn_rate=i_learn_rate,\n",
    "        b_learn_rate=b_learn_rate,\n",
    "        c_learn_rate=c_learn_rate,\n",
    "        i_l2_decay=i_l2_decay,\n",
    "        b_l2_decay=b_l2_decay,\n",
    "        c_l2_decay=c_l2_decay,\n",
    "        n_ins=n_ins,\n",
    "        batch_size=batch_size,\n",
    "        batch_op=batch_op,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    model_save(\n",
    "        i_model=i_model,\n",
    "        b_model=b_model,\n",
    "        c_model=c_model,\n",
    "        i_model_dir=i_model_dir,\n",
    "        b_model_dir=b_model_dir,\n",
    "        c_model_dir=c_model_dir,\n",
    "        n_class=n_class,\n",
    "        m_bag_op=m_bag_op,\n",
    "        m_clam_op=m_clam_op,\n",
    "        g_att_op=g_att_op,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clam_test(\n",
    "    n_class,\n",
    "    n_ins,\n",
    "    att_gate,\n",
    "    att_only,\n",
    "    mil_ins,\n",
    "    mut_ex,\n",
    "    test_path,\n",
    "    result_path,\n",
    "    result_file_name,\n",
    "    i_model_dir,\n",
    "    b_model_dir,\n",
    "    c_model_dir,\n",
    "    m_bag_op,\n",
    "    m_clam_op,\n",
    "):\n",
    "\n",
    "    i_trained_model, b_trained_model, c_trained_model = restore_model(\n",
    "        i_model_dir=i_model_dir,\n",
    "        b_model_dir=b_model_dir,\n",
    "        c_model_dir=c_model_dir,\n",
    "        n_class=n_class,\n",
    "        m_bag_op=m_bag_op,\n",
    "        m_clam_op=m_clam_op,\n",
    "        g_att_op=att_gate,\n",
    "    )\n",
    "\n",
    "    test_step(\n",
    "        n_class=n_class,\n",
    "        n_ins=n_ins,\n",
    "        att_gate=att_gate,\n",
    "        att_only=att_only,\n",
    "        mil_ins=mil_ins,\n",
    "        mut_ex=mut_ex,\n",
    "        i_model=i_trained_model,\n",
    "        b_model=b_trained_model,\n",
    "        c_model=c_trained_model,\n",
    "        test_path=test_path,\n",
    "        result_path=result_path,\n",
    "        result_file_name=result_file_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
